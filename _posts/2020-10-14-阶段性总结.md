---
layout: post
title: "阶段性总结"
date: 2020-10-14 17:23:37
image: '/assets/img/'
description: '忙碌了一个阶段来个总结8'
tags:
- Staged summary
categories:
- Summary
twitter_text: 'Do you know these?'

---

## To You

When I think of you, I'm reminded of the beautiful plains of Iowa.The distance between us is breaking my spirit. My time and experiences without you are meaningless to me. Falling in love with you was the easiest thing I have ever done. Nothing matters to me but you.And everyday I am alive, I'm aware of this. I loved you the day I met you, I love you today, and I will love you to rest of my life.

## 阶段性回顾

2020真的是特别的一年，人生也总是忙忙碌碌，不知不觉已然十月，这一年真的发生了好多，自己也真的长大了好多，懂得了很多道理，学会了很多事情，也渐渐成长为了一个独立强大的人。

研究生的路途虽不平坦，也总是觉得没有得到应有的专业能力，但痛苦也教会我如何辨人，教会我如何相处，教会我如何在逆境中保持阳光的心态，积极地去解决问题。我想这种内心的锤炼，或许才是意义所在。

最近第一篇难产的文章也算是投递出去了，是关于集成学习的，在这里有几点想拎一拎。

## 集成学习的一些知识点

集成学习在工业界中使用的还是十分广泛的，它可以使得模型的边界更加稳定，也可以降低过拟合的风险，bagging算法是在结果平均的基础上加上了随机选取特征的方法来构造决策树，所以可以并行去训练。

bagging算法的典型代表是随机森林，理论上越多的树效果会越好，但实际上有个上限，之后就只会上下浮动了。特征的重要程度是通过拿一些碎机噪音替代某些特征，然后看与正常数据训练出的模型相比发生了什么变化。

boosting的算法是串行的，在每一轮迭代上，对分错的数据在下一轮赋予更大的权重，最后对每一轮的分类器根据训练的误差根据公式计算出分类器权重最后加权。

stacking的算法很直接，把数据分成K折，把第一层训练集得出的预测结果，叠起来当作第二层的训练数据。把第一层模型对测试集得出的结果取平均作为第二层的测试数据。需要注意的是，第一层的基模型最好是强模型，第二层可以是一个简单的分类器。第一层的基模型数量不能太少，且最好是准而不同。

blending与stacking大致相同，区别在于训练集合不是通过K-fold的CV策略来获得的，而是采用了留出法CV。基模型用不想管的数据训练，对他们在测试集上的结果做平均。优点是简单，不会造成数据穿越，但对训练数据的利用低，也不够稳健。

StackNet是一种计算性的，可拓展的，分析性的meta-modeling框架。

评估上常用的ROC曲线，是通过动态调整阈值然后把分类结果打点画出来的，曲线下的面积被称为AUC值

## 不均衡问题

欠采样：缺点是未考虑样本的分布情况，采样过程的随机性可能会误删一些重要的信息。EasyEnsemble是随机抽取样本进行多次训练；BalanceCascade是先随机训练，对分类正确的样本不放回继续欠采样。基于KNN的NearMiss，通过不同策略选取少数类样本周围的多数样本点。

过采样：随机过采样，SMOTE是通过对少数样本计算K近邻，在特征空间中随机选取近邻来合成数据。Borderline-SMOTE只是为k邻近中又一半以上的多数类样本的少数累样本生成新样本，如果周围全为多数类样本，那么这个点会认定为噪声。基于K-means的过采样，就是在采样之前先进行聚类。

移动阈值：根据正负样本的比例来设定阈值

## 调参

Hyperopt 是一个批量调参的工具，对神经网络这种参数特别多的结构的时候挺有用的，有机会可以深入体验一下。对于GBDT、XGBoost、LightGBM的参数理解方面也可以多看看。

DCA是一个自编码的降噪工具

scScope

## LightGBM

GBDT是经典的利用弱分类器迭代的模型，xgboost的基本思想是通过对所有特征进行预排序，便于后续的节点分割。但是这样的算法耗费了大量的空间，计算分裂增益的时间消耗也不少。lightGBM在传统的GBDT上进行了算法优化：Histogram的决策树算法 ；GOSS单边梯度采样 ；EFB互斥特征捆绑 ；带深度限制的Leaf-wise叶子生长策略 ； 支持类别特征 ；支持并行 ；Cache命中率优化 （[详细](https://zhuanlan.zhihu.com/p/99069186)）

## 沃丁顿景观图

原始的横轴是细胞特征降到一维上的位置，纵轴是时间，像从山顶的仰视图

## 迁移学习

https://blog.ailemon.me/2020/08/03/transfer-learning-in-bioinformatics/

Surface protein imputation from single cell transcriptomes by deep neural networks（CTP-net）

## PCA和robust PCA

https://blog.csdn.net/u010510350/article/details/77725953

https://blog.csdn.net/universe_1207/article/details/102082446

机器学习150: https://zhuanlan.zhihu.com/p/213774840

特征选择与稀疏学习： https://www.jianshu.com/p/1ffc89a70c1d

单细胞测序工具： https://cloud.tencent.com/developer/article/1471483

理解优化：https://zhuanlan.zhihu.com/p/37108430

## CV小课堂

图片分类 -> 目标分割（像素级别分割） -> 目标检测（定位目标，确定目标在图像中的位置和大小） -> 目标识别（确定目标具体是什么） -> 目标跟踪（利用帧与帧之间时序关系建立运动模型追踪目标的运动轨迹）

图像分割就是根据某些规则把图片中的像素分成不同的部分（包含超像素，语义分割，实例分割，全景分割）

超像素：具有类似特征的像素集合

语义分割：把像素赋予一个类别标签

实例分割：类似objection detection，但检测出的是一个mask而不是bounding box

全景分割：是语义分割和实例分割的结合，在类别中进行实例的具体划分

## 投影

向量投影就是两个向量求内积然后除以投影方向的向量模长，利用垂线垂直于投影方向的性质变换形式后能得到一个投影矩阵，它的秩为1，代表把向量投影到一个直线上，幂等是因为多次投影不会改变结果

矩阵的投影是把向量的集合，也就是矩阵的列空间，投影到另外一个空间去，同理可以写成p = Pb的形式得到投影矩阵

## 矩阵乘法的四种理解

这个虽然很基础，但是对于乘法含义会有更深的了解

## 机器学习的数据需求

前段时间review给我的comments其中有一条问我，我的模型需要多少数据量才足够。其实这个问题在我最初接触机器学习的时候我也问过我自己，后来慢慢忘却，如今又被重提，那我就好好整理一下解答。

首先我们需要定义有效数据量，粒度是什么。然后我们去看数据量和特征的比例。

然后我们需要去看特征间的相关性，一般样本间和特征间的重复率都要比较低，数据是否有效。

数据的重要程度比模型更重要，确认来源性，确认显著特征是否正常，有没有人为经验选取的数据等等。

## 技术路线

框架可以有熟悉的感觉，但是基本公司内部都有自己的系统，所以进去再从头学也不迟

基础知识：数据结构，算法，操作系统等。在**编译原理**上，可以体会语法背后的实现（会接触到有限状态机，语法分析中的递归下降分析）；可以理解在淘宝京东这样的电商中营销规则的实现（实现一个配置系统和一个语言解析器，设计一个领域特定语言DSL来表达这些规则，比如JSON，SQL，HTML都算是DSL）；可以学到抽象语法树AST的生成，转化，优化并最后变成机器指令；看到寄存器中贪心算法的应用；图论中可达性分析死代码消除的实现。在**操作系统**上，程序的运行涉及到进程的管理，内存的管理，文件系统，用锁，条件变量，临界区来控制并发执行。**体系结构**上去了解计算机是如何工作的；高低电平配合与或非的逻辑电路门是如何实现二进制运算的；指令是如何被CPU执行的（CPU从内存或者缓存中取出指令放入指令寄存器，并对指令译码，将指令拆分成一系列的微操作和操作数，然后发出各种设备控制指令，执行微操作）在**组成原理**中学习底层的体系结构是如何对程序语言的帧栈和参数传递进行支持的。**数据结构和算法**能让我们了解到什么时候用红黑树实现的Map，什么时候后HashMap。最后可以去补补更上层的应用软件，比如计网，数据库等等。

特定领域的知识：是细分方向上的一些知识，假设想去从事游戏引擎的开发，那么图形学，渲染等等就是特定领域的知识。如果是做分布式存储，就需要了解分布式理论知识。或者是了解一些语言的底层原理，比如C++中过的STL，对象模型，虚函数表，智能指针；Java中的JVM，垃圾回收算法，Concurrent HashMap等。如果需要网络访问的话，HTTP，TCP等等，如果流量比较大的话还需要了解缓存，异步，消息队列，NoSQL等等。

至于一些其他的技能，应该是提升效率的，比如IDE，版本管理工具，SSH等等

## 矩阵补全

这学期学了一门课叫做子空间学习，是一门相当硬核的数学课程，其中有一个问题叫做矩阵补全，其应用场景非常丰富，所以在这里额外展开了解一下。

比如我们有一个用户A的观影历史数据矩阵，我们想要设计一个简单的算法来给他推荐他可能喜欢的电影。如果协同过滤算法，我们是找到和他最相似的一个用户B，然后把B看过A没有看过的电影推荐给A。本质上就是一个矩阵补全的问题。

其完备的理论是08年Candes的一篇论文，通过解一个凸优化问题实现一个低秩矩阵恢复。在图像恢复和推荐系统中都很有作用，矩阵分解就是其中一种办法（用A*B来近似不完全矩阵M）。低秩问题的求解是非凸的，我们一般用核范数（矩阵奇异值之和）去近似它。

在真实的情况中，这个矩阵不仅是低秩的，而且还是稀疏的。

## numpy中注意点

星乘是对应位置相乘，点乘是矩阵的内积。

有一个einsum的东西，叫做爱因斯坦求和约定，在速度和内存效率上拥有很大的优势。比如我们有一个行向量A和一个矩阵B，通常广播再对行求和的语句是`(A[:,np.newaxis]*B).sum(axis=1)`，但如果我们使用einsum，可以写作`np.einsum('i,ij->i',A,B)`，箭头左边是运算的两个块的轴（先列轴再行轴），右边是运算后的轴。消失的轴表示最后会沿着该轴做一次累加。在处理大量维度的时候可以使用...来表示我们不感兴趣的轴。

## 伪时推断方法

第一步是降维，然后进行聚类，然后在图上跑KNN或者MST，最后分配伪时间。分配时间的方法大致有主曲线+正交投影，聚类中心+正交投影，直径路径+PQ树，任意长度随机游走的转换概率，离源点的距离，通过航点进行距离细化。几个代表性的算法：TSCAN，Mpath，Slingshot。

tscan：

## PQ树

将节点分为两类分类讨论，P表示儿子可以任意顺序排列，Q表示其儿子可以从左到右或者从右到左排列。每个节点有关节点颜色（白色表示节点代表的子树中没有关键点，灰色表示节点代表的子树中部分是关键点，黑色表示子树中全是关键点）。最终的可行的方案是PQ树的先序遍历。被用来解决一些目标是寻找满足各种约束的顺序的问题。

## 图神经网络

社交网络，知识图谱，还有计算机视觉中目标识别的时候目标关节点构成，都可以看作是一个不规则的图结构，这些节点之间不是相互独立的，所以属于非欧空间。

经典的图算法有最小生成树，最短路径，拓扑排序，关键路径等。后来有一批人开始用概率图模型进行研究，再到后来就是现在比较流行的图神经网络。

图神经网络简称GNN，最早是2005年提出，09年在一位大佬的博士生论文中给出了理论基础，基于不动点理论。最初它还是用于解决严格意义上的图论问题的，13年LeCun的学生Bruna提出了基于频域和空域的卷积神经网络，极大地推动了应用的广泛。同期，14年Perozzi也受Word2Vec的启发提出了DeepWalk，由此延伸出了一套图嵌入表示的方法。

GNN的思想是不断地利用当前时刻邻居节点的隐藏状态作为部分输入来生成下一时刻中心节点的隐藏状态，直到每个节点的隐藏状态变化很小，视作是每个节点都知晓了其邻居的信息。此时只是把边作为传播的途径，隐藏状态之间信息共享也可能会导致最终节点的状态过于平滑。因此后来发展出Gated GNN（类似GRU，结合上了边信息，优化算法用BPTT）。

GCN是卷积图神经网络，空域的卷积操作定义在每个节点的连接关系上，频域类似于傅立叶变换后再进行卷积。两者都有非常多的论文和模型变体，用的方法数不胜数，但都大同小异。

最后我们需要把节点的表示变成整幅图的表示，这里可以采用可微池化的全连接层来实现。

至于不同模型的效果上来说，曾经有人做过实验，GCN的表现应该说是最出色的。在这个基础上可以加上注意力机制，也就是GAT模型，我也看了很多其他方法，考量之后最后选择了GraphSage。

## EDA软件

在电子CAD技术基础上发展起来的计算机系统软件，使得很多功能能够被计算机自动完成，便于专业人员提升设计效率

## RNA速度

在使用的时候，首先导入数据，这里可以为数据设定细胞种类的颜色。然后进行预处理，

## 分层随机抽样

先将总体各单位按照一定的标准分成各种类型，然后根据一定的比例确定每层中的具体数据，最后进行随机抽取。

## 推荐算法中方法变迁

传统机器学习是去记住常见高频的形式，逻辑回归模型其实在工业中就是一个类似金融风控领域中一个上亿条目的评分卡，所有的模式都依赖于人工输入，LR本身并不能发掘出新模式，它只负责评估各模式的重要性（通过交叉熵损失和SGD学到），反倒在正则过程中可以剔除一些罕见的模型。

为了避开“大众推荐”的红海内卷，拥抱“个性化精准推荐”的蓝海，我们需要去挖掘出低频长尾的模式。做法就是将粗粒度的概念拆解成一系列细粒度的特征，这样即使是LR也能具备拓展的能力。但这种特种工程的人工拆解费时费力，所以我们让算法自动地去做这些事（Embedding）。

面向Object的编程（OOP）的核心思想是将不同的实现隐藏在相同的接口后面。深度学习中会将特征，函数转化为待优化变量，这里的embedding和传统矩阵分解的推荐方法中的隐变量相似。通过Embedding可以将推荐算法从精准匹配转化为模糊查找，进而能够更好地“举一反三”。

后来，就到了图计算和知识图谱的阶段，ID类特征变成了图上的节点，并且这些信息之间彼此关联，带来了信息的传递。

## 矩阵的几个概念

稀疏矩阵，一般是指其中的0元素个数远大于非0元素，且分布无规律。稀疏表示，指的是寻找一个系数矩阵A和一个字典矩阵B，使得B*A尽可能地还原X，且A尽可能地稀疏，这里的A就是X的稀疏表示。所谓字典学习，又称稀疏编码，是为普通稠密表达的样本找到合适的字典，使得学习任务简化，降低模型复杂度。

## 迁移学习

利用数据和领域之间存在的相似性，把之前学习到的知识，应用于新的未知领域。如果领域之间不存在很好的相似性，那么就会发生负迁移，导致学习的结果很差。

## 多任务学习

对于复杂的问题，如果我们把它分解为简单且相互独立的子问题来单独解决，然后再合并结果的做法其实是不正确的。因为这会忽略很多关联性。那么怎么做呢？这就是多任务学习，在学习过程中共享信息，从而取得更好的泛化效果。

这是迁移学习的一种，主任务使用相关任务的训练信号作为推导偏差来提升主任务的泛化效果。MTL中共享有两种方式：一种是基于参数的共享（神经网络），一种是基于约束的共享（均值或联合特征创建一个常见的特征集合）

有效的原因：引入噪声提升了泛化效果，不同任务的相互作用帮助隐含层逃离局部最小值，多任务并行学习提升了共享层的学习速率。

## 联系

多标签学习是多任务学习中的一种，建模多个label之间的相关性，共享多个类别之间的数据特征。

多类别学习是多标签学习中的一种，对多个相互独立的类别进行建模。

## 图像子空间聚类和分类算法研究

这是一篇浙大的研究生论文，子空间学习研究中主要分为：核子空间方法，半监督子空间方法，张量子空间方法，增量子空间方法和非负与稀疏子空间方法。

核子空间：主要就是通过核技巧对经典的算法进行转化，变为只需要内积形式的计算。基本思想是将原始空间的数据映射到更高维或是无穷维的空间，在高维空间中形成一个高度非线性的曲面，使得原本线性不可分的数据变为线性可分。散度矩阵（也叫类内离散度矩阵，等于协方差矩阵乘以（n-1），n是样本的个数，假设特征维度为d，协方差矩阵是一个d*d的半正定矩阵），

## 子空间学习essay

多任务学习是一种通过在训练中迁移有用信息，而在学习效果和鲁棒性上都有良好表现的框架（MTL）。但这些任务必须要是相关的，否则就会发生负迁移（效果变差）。早期的MTL研究认为相关的任务共享一个稀疏的特征集合，或者任务参数比较相近，或者共享一个先验，或者在一个相同的低维子空间或者低维流型中。

我们用子空间聚类的方法去处理法任务，假设所有的任务参数都来自一个子空间的集合，而相关的任务参数落在同一个子空间中，此时子空间的个数和维数都是未知的。通过引入隐藏任务，即使任务数小于子空间的秩，我们也能把所有的任务用其所在簇的其他任务来表示。

## IO多路复用

所有的IO设备都被抽象为file的概念，理解上就是一个N byte的序列。常用接口有：打开文件（open），改变读写位置（seek），文件读写（read, write），关闭文件（close）。

Linux中标识文件身份的东西叫文件描述符，就是一个数字代号。有了文件描述符，进程可以对文件一无所知，都交给操作系统打理。如果服务端需要同时处理高并发的情况，在经典的阻塞式IO下就会遇到问题，此时如果采用多线程的话，成千上万的进程大量创建和销毁会严重影响系统性能。

所以如果在有多路请求的情况下，我们拿到了一堆文件描述符，调用某个函数（select, poll, epoll）告诉内核当文件可读的时候返回给我，这种过程被叫做IO多路复用。

接下来我们来具体看一下这三剑客：

> 1. select：Linux内核对集合的大小做了限制（1024），当其返回后我们仅仅能知道有些文件描述符可以读写了，但不知道是哪一个，因此需要再遍历一遍。
> 2. poll：优化了1024个的限制，但它和select一样，都会随着监控的文件描述符的数量增加而性能下降。
> 3. epoll：做到了只操作那些有变化的文件描述符，与内核共享了一块内存用于保存可读可写的文件描述符集合，减少了拷贝的开销。它还实现了无需再遍历的功能，直接记下可读写的描述符。

## 关于抄袭与一位朋友的辩论

Bumble的创始人是从著名约炮软件tinder出来的一名员工，契机是性骚扰案件，该员工把这事作为噱头进行碰瓷营销（已经被列为经典案例），进而出来创业。

Bumble几乎与前身tinder一模一样，左右滑挑选对象。除了tinder，市面上很多软件，浅显如QQ扩列，陌陌，探探都是这种UI模式，谁先谁后不予考证，因为没有意义。不同点在于bumble提倡“女性主动得跨出第一步，而不是被动等待。”，所以在玩法的匹配过程中必须由女性先发起聊天，否则他们的匹配就是无效的。其原因是因为根据Tinder的数据，更多的男生在打招呼的时候只会简单得说一句“Hi”，而女生则是更耐心得会用一句长句来开启对话，这样的破冰成功率会高出不少。

由此可见，bumble也并非原创，而且它和tinder撕逼撕的更难看。但这样的抄袭完全是不正确的吗？也未必，如果你对app软件开发稍作了解，你可以知道，几乎每一个成功的产品背后都有抄袭的影子，腾讯就不说了，你可以看一下食品行业的奥利奥抄袭之路，大名鼎鼎的facebook疯狂抄袭初创企业的功能数不胜数，抄袭不成就收购。

为什么这样呢，这就引出了市场存在的意义，市场和用户需要的是一个完美的产品，而只有做到行业最好，才能存活下来。 为什么我们不用淘宝的来往，电信的易信，小米的米聊，早在微信之前好多好多年，市面上就有各种各样的即时通讯软件，几乎都一模一样。为什么我们网上购物不用腾讯的拍拍，而是用淘宝和京东。这其中有很多原因，但杀出血路的不一定是第一个，事实上绝大多数也不是第一个，但它一定是行业的极致，极致到无人超越。在信息时代，idea是最不值钱的东西，因为所有东西都会遍地开花，你能做的是把事情做到极致，永远都会有人想代替你，但如果你极致，别人就没有兴趣来尝试了，除非收购你。

说回她说，首先app和你经常使用的文学，音乐上的抄袭例子是完全不同的，我不认为可以用来做类比。创业抄袭是一件必然会发生的事，而这件事的存在是人性使然，也促进了行业的竞争发展，有正向作用。其次，app抄袭层出不穷，很多公司都会设立技术门槛来防止被轻易超越，除了技术，最显然带不走的就是用户，她说是国内用户，而且不同点主打高质量社交，它有简洁舒适的用户体验和更高的用户质量审查，这都是经过市场检验的，而且仔细观察可以发现，里面有很多玩法也是不一样。

其实不论什么产品，背后总会有说是抄袭的，但我几乎没有看见对她说的讨伐报道，你可以搜一搜。

下面是知乎测评用户Meika大喵子的分类

> 一类是**积目、花田、轻聊、Soul这类以大规模抢占市场为战略的APP**，这些产品的主要问题就是功能太多，界面太乱，给人的感觉就是什么功能都想做，但什么都不太灵。
>
> 另一类就是**走高冷小众路线，强调B格、调性、用户层次的产品，例如zoosk，league，恋爱圈，queenier等等**，这类产品要么国外引进，要么创始团队普遍具有海外留学经历，情怀党居多，多数致力于打造一个高净值人群的社交空间，但往往一直在用户数量这一关停滞不前。
>
> 当然「她说」本质上也属于第二类，但相对来讲，功能和推广形式都还比较接地气，我觉得是第二类产品值得学习借鉴的。

这段话说的是当你看待一个产品，他有很多很多你没有考虑的角度决定它是否成功，而你可能看不见，你目前抓住的事物，其实是很次要的。最后总结一下，首先要全面去看一个产品哪里在创新哪里在抄袭，为什么会成功；其次存在着的产品肯定会有它偏好的受众，用户开心，它就是好产品。如果没有抄袭，就没有良性的创业环境。

## 放假杂记整理

人有23对染色体，31亿个碱基对，只有2.9%是编码的，所以那些非编码的存在意义是什么

表观遗传信息指的是DNA甲基化，组蛋白修饰，核小体位置等

一个基因可以有多个剪切体

公司的价值观（why you come），了解岗位，在介绍项目的时候从（Situation什么时候为什么，Task目标想法，Result结果数据，Action怎么做担任什么角色）

GraphSage这篇文章值得读一读

mRNA片段的结构：Read1 + Barcode + UMI + polyA

一些命令：nvcc -V（GPU版本），nvidia -smi（GPU当前状态），env（显示环境变量及其赋值），env ｜ grep LD（正则寻找匹配），which jupyter（显示当前使用的jupyter路径），dpkg命令（安装、删除、构建和管理Debian的软件包）

零空间：是线性映射中的概念，指零的原像空间。矩阵算子（线性算子）的零空间（线性子空间）也叫它的核。

希尔伯特空间：是欧几里得空间到无限维的一个推广。

SQP最优化算法，高斯牛顿法，类牛顿法，施密特正交化，panglaodb.se是一个数据库，投影矩阵A（ATA）逆AT

QR分解

barcode（样本标签）, read（读段），UMI（单分子标签）, polyA（3末端维持mRNA作为翻译模板的活性并增加本身的稳定性）

反向传播，梯度回流，像空间，核空间，A-invariant

再参数化是自编码器里的一个技巧，从单位高斯中随机采样

栈编码器里用到了权重捆绑

设计测试用例：等价类划分，边界测试

The mythos of model interpretability

## 理财

回撤，重仓：

茅台是中国内股第一名，选股票就是在选公司，

中国股市的平均涨幅叫做大盘，就是沪深走势，上涨的股票大于下降的股票就称为牛市。

招商银行是银行板块里市值最高的。

技术面：K线，均线（平均走线，五日/十日/二十日/六十日）

死叉：代表股票大概率要跌（五和十往下交叉）

金叉：代表股票即将上涨（五和十往上交叉）

乌云盖顶：红色K线小于绿色的一半，股票即将下跌

基本面：财报，在巨潮资讯网上找

Muddy Waters（浑水机构）是美国的一家监管公司，无人机调查出辉山乳业财务作假

怎么看财报：看重要提示中会计师的审计意见情况

疫情：药，口罩，柴米油盐肉，疫苗

抬轿子：基金之间粉饰业绩的行为，

方塘温度

保险避坑指南：

## 硕士论文

基于机器学习模型的RNA速度预测及其下游应用

TCBB会议，RECOMB（五月初在华盛顿），ISMB（七月21-25在瑞士），ECCB，PSB，KDD（2月3日），ACM-BCB（1.15）

选题符合研究方向，选题的价值性，现实意义，小题大做

从主要文献中铺开

## spektral

Graph被写成一个类，属性：邻接矩阵a，结点特征矩阵x，边特征矩阵e，标签y

apply(transform) 是对每个图都原地执行transform操作，map是返回一个处理后的list，filter(function)是把function作用是False的图原地移除

## Transformer

Transformer技术被谷歌提出，将序列分解为矩阵计算，来提高训练速度。淘汰了lstm成为了最受欢迎的技术，attention是一种